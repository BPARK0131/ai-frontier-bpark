import os
import re
import time
import pandas as pd
import streamlit as st
from sqlalchemy import create_engine
from sqlalchemy.types import String, Integer, DateTime
from transformers import AutoTokenizer
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.utilities import SQLDatabase
from langchain.chains import create_sql_query_chain
from langchain_community.tools.sql_database.tool import QuerySQLDataBaseTool
from langchain.schema import Document
from langchain.vectorstores import FAISS
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever
from langchain_openai import ChatOpenAI
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain_text_splitters import RecursiveCharacterTextSplitter
import nest_asyncio
import asyncio

#API_KEY = st.secrets["OPENAI_API_KEY"]

# Streamlit ì• í”Œë¦¬ì¼€ì´ì…˜ ì œëª© ì„¤ì •
st.title("ğŸ’¡ ê³ ì¥ ë¡œê·¸ ë¶„ì„ ë° ë‹µë³€ ë„ìš°ë¯¸v ")

# API_KEY.txt íŒŒì¼ì„ í™˜ê²½ ë³€ìˆ˜ë¡œ ë¶ˆëŸ¬ì˜¤ëŠ” í•¨ìˆ˜ ì •ì˜
def load_config(file_path):
    with open(file_path) as f:
        for line in f:
            key, value = line.strip().split('=')
            os.environ[key] = value

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
load_config('/workspaces/ai-frontier-bpark/API_KEY.txt')
#os.environ["OPENAI_API_KEY"] = st.secrets["OPENAI_API_KEY"]
os.environ["TOKENIZERS_PARALLELISM"] = "false"

gip_base_url = "https://api.platform.a15t.com/v1"

# CSV ë°ì´í„° ì „ì²˜ë¦¬ ë° ë¡œë“œ (ìºì‹œ ì ìš©)
@st.cache_data
def load_and_preprocess_data(file_path):
    df = pd.read_csv(file_path, delimiter=",", quotechar='"')
    df.fillna("", inplace=True)
    df = df.apply(lambda x: x.str.strip() if x.dtype == "object" else x)
    df.drop_duplicates(inplace=True)
    df['event_time'] = pd.to_datetime(df['event_time'])
    df['Event Year'] = df['event_time'].dt.year
    df['Event Quarter'] = df['event_time'].dt.to_period('Q').astype(str)
    df['Event Month'] = df['event_time'].dt.strftime('%Y-%m')
    df['gen'] = df['gen'].astype('category')
    df['role'] = df['role'].astype('category')
    df['syslog'] = df['syslog'].str.lower()
    df['keyword'] = df['keyword'].str.lower().replace({
        "link down": "link issue",
        "temperature alarm": "temp alarm"
    })
    df['Event Date'] = df['event_time'].dt.strftime('%Y-%m-%d %H:%M:%S')
    df['after_action'] = df['after_action'].apply(lambda x: x[:500] if len(x) > 500 else x)
    return df

file_path = "/workspaces/ai-frontier-bpark/dummy_data_241018.csv" #Testìš© ê²½ë¡œ
df = load_and_preprocess_data(file_path)

# SQLite ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ë° ì„¤ì • (ìºì‹œ ì ìš©)
@st.cache_resource
def create_sql_database(df):
    engine = create_engine("sqlite:///fault_data.db")
    df.to_sql("fault_events", con=engine, if_exists="replace", index=False, dtype={
        'event_time': DateTime,
        'gen': String,
        'vendor': String,
        'role': String,
        'device_name': String,
        'syslog': String,
        'card': String,
        'unit_name': String,
        'alarm_group': String,
        'keyword': String,
        'service_effect': String,
        'after_action': String,
        'action_result': String,
        'Event Year': Integer,
        'Event Quarter': String,
        'Event Month': String
    })
    return SQLDatabase(engine)

db = create_sql_database(df)

# ì„ë² ë”© ë° ë²¡í„° ìŠ¤í† ì–´ ìƒì„± (ìºì‹œ ì ìš©)
@st.cache_resource
def create_vector_store(_docs):
    model_name = "intfloat/multilingual-e5-large-instruct"
    hf_embeddings = HuggingFaceEmbeddings(
        model_name=model_name,
        model_kwargs={"device": "cpu"},
        encode_kwargs={"normalize_embeddings": True},
        show_progress=True
    )
    # ë²¡í„° ìŠ¤í† ì–´ ë¡œë”© ìµœì í™”
    if os.path.exists("faiss_index.faiss"):
        vectorstore = FAISS.load_local("faiss_index.faiss", hf_embeddings, allow_dangerous_deserialization=True)
    else:
        vectorstore = FAISS.from_documents(documents=_docs, embedding=hf_embeddings)
        vectorstore.save_local("faiss_index.faiss")
    return vectorstore, hf_embeddings

# í…ìŠ¤íŠ¸ ë¶„í• ê¸° ìƒì„± ë° ë¬¸ì„œ ë¶„í• 
text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(
    tokenizer=AutoTokenizer.from_pretrained("intfloat/multilingual-e5-large-instruct"),
    chunk_size=400,
    chunk_overlap=30
)
docs = [Document(page_content=row['syslog'], metadata={"event_time": row['event_time']}) for _, row in df.iterrows()]
splitted_docs = text_splitter.split_documents(docs)
vectorstore, hf_embeddings = create_vector_store(_docs=splitted_docs)

# ê²€ìƒ‰ê¸° ì„¤ì • (BM25 ë° FAISS)
bm25_retriever = BM25Retriever.from_documents(splitted_docs)
bm25_retriever.k = 3
faiss_retriever = vectorstore.as_retriever(search_type='similarity', search_kwargs={'k': 3})

# ì•™ìƒë¸” ê²€ìƒ‰ê¸° ìƒì„±
ensemble_retriever = EnsembleRetriever(
    retrievers=[bm25_retriever, faiss_retriever],
    weights=[0.5, 0.5],
    search_type="similarity"
)

# LLM ì„¤ì • ë° SQL ì¿¼ë¦¬ ê´€ë ¨ ì²´ì¸ ì •ì˜
sql_llm = ChatOpenAI(model_name="azure/openai/gpt-4o-mini-2024-07-18",
                    streaming=True, callbacks=[StreamingStdOutCallbackHandler()],
                    temperature=0.2, base_url=gip_base_url)

# SQL ì¿¼ë¦¬ ìƒì„± ë° ì‹¤í–‰ íˆ´ ì„¤ì •
execute_query = QuerySQLDataBaseTool(db=db)
write_query = create_sql_query_chain(sql_llm, db)

# ì§ˆë¬¸ì— ë”°ë¥¸ ì¿¼ë¦¬ ìƒì„± í…œí”Œë¦¿ ì„¤ì •
answer_prompt = PromptTemplate.from_template(
    """Given the following user question, corresponding SQL query, and SQL result, answer the user question.
If you don't know the answer, just say that you don't know.
Answer in Korean.

Question: {question}
SQL Query: {query}
SQL Result: {result}
Answer: """
)
answer = answer_prompt | sql_llm | StrOutputParser()

# SQL ì¿¼ë¦¬ ì •ì œ í•¨ìˆ˜ ì •ì˜
def clean_and_validate_query(query):
    clean_query = re.sub(r'```sql|```|SQLQuery:', '', query).strip()
    return clean_query.replace("\n", " ").strip()

# ë¼ìš°íŒ…ì„ ìœ„í•œ Runnable ì„¤ì •
router_prompt = PromptTemplate(
    input_variables=["input"],
    template="ë‹¤ìŒ ì§ˆë¬¸ì´ SQL ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì¡°íšŒí•  ìˆ˜ ìˆëŠ”ì§€ ì—¬ë¶€ë¥¼ íŒë‹¨í•˜ì„¸ìš”.\nì§ˆë¬¸: {input}\nSQL ì¡°íšŒê°€ ê°€ëŠ¥í•˜ë‹¤ë©´ 'SQL'ì´ë¼ê³  ë‹µí•˜ê³ , ê·¸ë ‡ì§€ ì•Šë‹¤ë©´ 'RAG'ë¼ê³  ë‹µí•˜ì„¸ìš”."
)
router_llm = ChatOpenAI(model_name="azure/openai/gpt-4o-mini-2024-07-18",
                    streaming=True, callbacks=[StreamingStdOutCallbackHandler()],
                    temperature=0.5, base_url=gip_base_url)
router_runnable = router_prompt | router_llm | StrOutputParser()

# RAG í”„ë¡¬í”„íŠ¸ ìƒì„± ë° ì²´ì¸ ì •ì˜
rag_prompt = PromptTemplate.from_template(
    """You are an assistant for question-answering tasks.
Use the following pieces of retrieved context to answer the question.
If you don't know the answer, just say that you don't know.
Answer in Korean.

#Question:
{question}
#Context:
{context}

#Answer:"""
)
rag_llm = ChatOpenAI(model_name="azure/openai/gpt-4o-mini-2024-07-18",
                    streaming=True, callbacks=[StreamingStdOutCallbackHandler()],
                    temperature=0, base_url=gip_base_url)
rag_chain = (
    {"context": ensemble_retriever, "question": RunnablePassthrough()}
    | rag_prompt
    | rag_llm
    | StrOutputParser()
)

# ì¿¼ë¦¬ ì‘ì„± ë‹¨ê³„ (RunnablePassthrough)
query_runnable = RunnablePassthrough.assign(query=write_query)

# ìµœì¢… ì²´ì¸ ë¼ìš°íŒ…
multi_prompt_chain = RunnableLambda(
    lambda inputs: query_runnable.invoke({"question": inputs["input"]})
    if router_runnable.invoke(inputs) == "SQL"
    else rag_chain.invoke(inputs["input"])
)

# ìµœì¢… ë‹µë³€ì„ ìœ„í•œ í•¨ìˆ˜ ì •ì˜
def get_answer(user_input):
    result = multi_prompt_chain.invoke({"input": user_input})
    if isinstance(result, str) and result == "ì§€ì›ë˜ì§€ ì•ŠëŠ” ìš”ì²­ì…ë‹ˆë‹¤. SQL ì¿¼ë¦¬ë§Œ ì§€ì›ë©ë‹ˆë‹¤.":
        return result
    elif "SQL" in router_runnable.invoke({"input": user_input}):
        write_query_result = result
        write_query_result["query"] = clean_and_validate_query(write_query_result["query"])
        sql_result = execute_query.invoke({"query": write_query_result["query"]})
        final_answer_input = {
            "question": user_input,
            "query": write_query_result["query"],
            "result": sql_result
        }
        final_answer = answer.invoke(final_answer_input)
        return f"(SQL ì¡°íšŒë¥¼ í†µí•œ ë‹µë³€ì…ë‹ˆë‹¤.)\n\n{final_answer.strip()}"
    else:
        return f"(RAG ì¡°íšŒë¥¼ í†µí•œ ë‹µë³€ì…ë‹ˆë‹¤.)\n\n{result.strip()}"



# ìµœì¢… ë‹µë³€ì„ ìœ„í•œ ë¹„ë™ê¸° í•¨ìˆ˜ ì •ì˜
async def get_answer(user_input):
    return await multi_prompt_chain({"input": user_input})

# nest_asyncio ì ìš©
nest_asyncio.apply()

# Streamlit ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤
user_question = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:")

if st.button("ì§ˆë¬¸í•˜ê¸°"):
    if user_question:
        with st.spinner('ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤...'):
            # ë¹„ë™ê¸° í•¨ìˆ˜ì—ì„œ asyncio.run ëŒ€ì‹  asyncio.create_taskë¥¼ ì‚¬ìš©í•˜ì—¬ ì†ë„ í–¥ìƒ
            loop = asyncio.get_event_loop()
            return_answer = loop.run_until_complete(get_answer(user_question))
        
        st.markdown("#### ğŸ“œ ì§ˆë¬¸:")
        st.write(user_question)

        st.markdown("#### ğŸ“œ ë‹µë³€:")
        st.write(return_answer)

